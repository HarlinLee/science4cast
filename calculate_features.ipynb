{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "calculate_features.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Environment (conda_tensorflow2_p37)",
      "language": "python",
      "name": "conda_tensorflow2_p37"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarlinLee/science4cast/blob/main/calculate_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfPvBFFE5qC2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy import sparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from datetime import date, datetime\n",
        "import time\n",
        "import networkx as nx\n",
        "from networkx.algorithms.centrality import katz_centrality\n",
        "import json\n",
        "import os\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "DRIVE_PATH = \"./\"\n",
        "day_origin = date(1990,1,1)\n",
        "NUM_OF_VERTICES = 64719\n",
        "data_source = os.path.join(DRIVE_PATH,'competition_data', 'CompetitionSet2017_3.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW6b1Yeu5H01",
        "scrolled": true
      },
      "source": [
        "def get_sparse_from_year(yy, edges, days=None):\n",
        "    if not days:\n",
        "        days = (date(yy,12,31)-day_origin).days\n",
        "        \n",
        "    return get_sparse_from_edges(edges[edges[:,2] < days])\n",
        "\n",
        "def get_sparse_from_edges(edges):\n",
        "    adj = sparse.csr_matrix(\n",
        "        (np.ones(2*len(edges)), \n",
        "        (\n",
        "            np.hstack((edges[:,0], edges[:,1])),\n",
        "            np.hstack((edges[:,1], edges[:,0]))\n",
        "        )), \n",
        "        shape = (NUM_OF_VERTICES,NUM_OF_VERTICES),\n",
        "        dtype = np.uint16\n",
        "    )\n",
        "\n",
        "    adj.setdiag(0) \n",
        "    adj.eliminate_zeros()\n",
        "    \n",
        "    return adj.asfptype()\n",
        "\n",
        "def get_edges_from_sparse(mat):\n",
        "    edges = sparse.triu(mat).nonzero()\n",
        "    return np.array([edges[0], edges[1]]).T\n",
        "\n",
        "def orgainze_edges(edges):\n",
        "    return get_edges_from_sparse(get_sparse_from_edges(edges))\n",
        "\n",
        "def create_training_data(full_graph,year_start,years_delta,edges_used=500000,vertex_degree_cutoff=10):\n",
        "    \"\"\"\n",
        "    :param full_graph: Full graph, numpy array dim(n,3) [vertex 1, vertex 2, time stamp]\n",
        "    :param year_start: year of graph\n",
        "    :param years_delta: distance for prediction in years (prediction on graph of year_start+years_delta)\n",
        "    :param edges_used: optional filter to create a random subset of edges for rapid prototyping (default: 500,000)\n",
        "    :param vertex_degree_cutoff: optional filter, for vertices in training set having a minimal degree of at least vertex_degree_cutoff  (default: 10)\n",
        "    :return:\n",
        "\n",
        "    all_edge_list: graph of year_start, numpy array dim(n,2)\n",
        "    unconnected_vertex_pairs: potential edges for year_start+years_delta\n",
        "    unconnected_vertex_pairs_solution: numpy array with integers (0=unconnected, 1=connected), solution, length = len(unconnected_vertex_pairs)\n",
        "    \"\"\"\n",
        "\n",
        "    years = [year_start,year_start+years_delta]    \n",
        "    adjs, days = [], []\n",
        "\n",
        "    for yy in years:\n",
        "        print('    Create Graph for ', yy)\n",
        "        days_curr = (date(yy,12,31)-day_origin).days\n",
        "        days.append(days_curr)\n",
        "        \n",
        "        adj = get_sparse_from_year(yy, full_graph, days_curr)\n",
        "        adjs.append(adj)\n",
        "        \n",
        "        print('    num of edges: ', adj.count_nonzero()//2)\n",
        "\n",
        "    ## Create all edges to be predicted\n",
        "    all_degs = np.array(adjs[0].sum(0))[0]\n",
        "    all_vertices = np.array(range(NUM_OF_VERTICES))\n",
        "    vertex_large_degs = all_vertices[all_degs>=vertex_degree_cutoff] \n",
        "    # use only vertices with degrees larger than vertex_degree_cutoff.\n",
        "    \n",
        "    ## get all positive examples\n",
        "    all_edges_after = full_graph[(days[0]<=full_graph[:,2]) & (full_graph[:,2]<days[-1])]\n",
        "    all_edges_after = all_edges_after[np.all(np.isin(all_edges_after[:,:2], vertex_large_degs), axis=1)]    \n",
        "    all_edges_after = orgainze_edges(all_edges_after)\n",
        "    \n",
        "    print(len(all_edges_after))\n",
        "    \n",
        "    ## get some negative examples\n",
        "    unconnected_vertex_pairs = []\n",
        "    np.random.seed(0)\n",
        "    \n",
        "    while len(unconnected_vertex_pairs) < max(edges_used-len(all_edges_after), len(all_edges_after)):        \n",
        "        v1,v2 = np.random.choice(vertex_large_degs, 2)\n",
        "\n",
        "        if (v1 != v2) and (not adjs[0][v1,v2]) and (not adjs[-1][v1,v2]):\n",
        "            unconnected_vertex_pairs.append((v1,v2))\n",
        "            \n",
        "    unconnected_vertex_pairs = orgainze_edges(np.array(unconnected_vertex_pairs))\n",
        "    \n",
        "    unconnected_vertex_pairs_solution = np.array([1]*len(all_edges_after)+[0]*len(unconnected_vertex_pairs))        \n",
        "    unconnected_vertex_pairs = np.vstack((all_edges_after[:, :2], unconnected_vertex_pairs))\n",
        "         \n",
        "    print('Number of unconnected vertex pairs for prediction: ', len(unconnected_vertex_pairs_solution))\n",
        "    print('Number of vertex pairs that will be connected: ' , sum(unconnected_vertex_pairs_solution))\n",
        "    print('Ratio of vertex pairs that will be connected: ' , sum(unconnected_vertex_pairs_solution)/len(unconnected_vertex_pairs_solution))\n",
        "\n",
        "    return get_edges_from_sparse(adjs[0]), unconnected_vertex_pairs, unconnected_vertex_pairs_solution"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4mytkoH5H03",
        "scrolled": true
      },
      "source": [
        "def compute_network_matrices(mat):\n",
        "    all_degs = mat.sum(0).T\n",
        "    \n",
        "    l = sparse.linalg.eigsh(mat, k=1, return_eigenvectors=False)\n",
        "    katz = katz_centrality(nx.Graph(mat), alpha=1/(l[0]+1))\n",
        "    k = pd.DataFrame.from_dict(katz, orient='index')\n",
        "    \n",
        "    mat = mat.dot(mat)\n",
        "    mat.eliminate_zeros()\n",
        "    \n",
        "    all_degs2 = mat.sum(0).T    \n",
        "    \n",
        "    return (all_degs, k, mat, all_degs2)\n",
        "    \n",
        "def compute_network_properties(mats, vlist):\n",
        "    \"\"\"\n",
        "    Computes hand-crafted properties for all vertices in vlist\n",
        "    \"\"\"\n",
        "    all_degs, k, mat, all_degs2 = mats\n",
        "    \n",
        "    all_properties=[]  \n",
        "    \n",
        "    d0 = all_degs[vlist[:,0]]\n",
        "    d1 = all_degs[vlist[:,1]]\n",
        "    div = np.multiply(d0, d1)\n",
        "    \n",
        "    all_properties.append(d0/all_degs.max())\n",
        "    all_properties.append(d1/all_degs.max())\n",
        "    all_properties.append(div/(all_degs.max()**2)) #Preferential Attachment\n",
        "\n",
        "    all_properties.append(k.loc[vlist[:,0]].values)\n",
        "    all_properties.append(k.loc[vlist[:,1]].values)\n",
        "\n",
        "    ####\n",
        "    m = mat[vlist[:,0], vlist[:,1]].T\n",
        "    all_properties.append(m/m.max()) #Common neighbours\n",
        "\n",
        "    all_properties.append(np.divide(m, div, out=np.zeros_like(m), where=div!=0)) #Leicht-Holme-Newman Index\n",
        "    div = np.sqrt(div)\n",
        "    all_properties.append(np.divide(m, div, out=np.zeros_like(m), where=div!=0)) #Salton Index (cosine similarity)\n",
        "    div = np.minimum(d0, d1)\n",
        "    all_properties.append(np.divide(m, div, out=np.zeros_like(m), where=div!=0)) #Hub Promoted Index\n",
        "    div = np.maximum(d0, d1)\n",
        "    all_properties.append(np.divide(m, div, out=np.zeros_like(m), where=div!=0)) #Hub Depressed Index\n",
        "    div = d0 + d1\n",
        "    all_properties.append(np.divide(m, div, out=np.zeros_like(m), where=div!=0)) #SÃ¸rensen Index\n",
        "    div = d0 + d1 - m\n",
        "    all_properties.append(np.divide(m, div, out=np.zeros_like(m), where=div!=0)) #Jaccard coefficient\n",
        "    \n",
        "    d0 = all_degs2[vlist[:,0]]/all_degs2.max()\n",
        "    d1 = all_degs2[vlist[:,1]]/all_degs2.max()\n",
        "\n",
        "    all_properties.append(d0)\n",
        "    all_properties.append(d1)\n",
        "    all_properties.append(np.multiply(d0, d1))\n",
        "\n",
        "    return np.squeeze(np.array(all_properties))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaO7dcBNhG0b"
      },
      "source": [
        "def create_nx_features(nx_alg, yy, graph, vlist):\n",
        "    adj = get_sparse_from_year(yy, graph)\n",
        "    G = nx.Graph(adj)\n",
        "\n",
        "    p = nx_alg(G, iter(vlist))\n",
        "    p = pd.DataFrame(p)\n",
        "        \n",
        "    return p.loc[:,2].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX2Y2mLhhG0c"
      },
      "source": [
        "def link_examples_to_features(link_examples, embedding, binary_operator):\n",
        "    return np.sum(binary_operator(embedding[link_examples[:,0]],\n",
        "                                   embedding[link_examples[:,1]]), axis=1)\n",
        "\n",
        "def operator_hadamard(u, v):\n",
        "    u_norm = np.sqrt(np.sum(np.power(u, 2), axis=1))\n",
        "    v_norm = np.sqrt(np.sum(np.power(v, 2), axis=1))\n",
        "    \n",
        "    return np.multiply(u / u_norm.reshape(-1,1),\n",
        "                       v / v_norm.reshape(-1,1))\n",
        "\n",
        "def operator_l1(u, v):\n",
        "    return np.abs(u - v)\n",
        "\n",
        "def operator_l2(u, v):\n",
        "    return (u - v) ** 2/u.shape[1]\n",
        "\n",
        "def operator_avg(u, v):\n",
        "    return (u + v) / 2\n",
        "\n",
        "def operator_concat(u, v):\n",
        "    return np.concatenate((u,v), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvZznxVPhG0c"
      },
      "source": [
        "def compute_embedding_features(yy, edges, bin_ops):\n",
        "    embed_features = []\n",
        "    with open(os.path.join(DRIVE_PATH, 'node2vec_embeddings', 'node2vec-'+str(yy) +'.pkl'), \"rb\") as output_file:\n",
        "        node_embeddings = pickle.load(output_file) # (64719, 128)\n",
        "        for bin_op in bin_ops:\n",
        "            embed_features.append(link_examples_to_features(edges, node_embeddings, bin_op))\n",
        "\n",
        "    return np.squeeze(np.array(embed_features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUZXxVFGhG0d"
      },
      "source": [
        "def save_feature(mat, fname, path=DRIVE_PATH):\n",
        "    with open(os.path.join(path, 'features', fname+'.npz'), \"wb\") as f:\n",
        "        #pickle.dump(mat, f)\n",
        "        np.savez_compressed(f, a=mat)\n",
        "    return 0\n",
        "\n",
        "def load_feature(fname, path=DRIVE_PATH):\n",
        "    with np.load(os.path.join(path, 'features', fname+'.npz')) as dat:\n",
        "        return dat['a']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR5XNH08hG0d"
      },
      "source": [
        "def generate_train(data, batch_size):\n",
        "    d0, d1 = data\n",
        "    while True:\n",
        "        idx0 = np.random.choice(len(d0), batch_size)\n",
        "        yield d0[idx0], d1[idx0]\n",
        "        \n",
        "def generate_test(data, batch_size):\n",
        "    data_len = len(data)\n",
        "    idx_start = 0\n",
        "    while True:\n",
        "        idx0 = range(idx_start, min(idx_start+batch_size, data_len))\n",
        "        idx_start += batch_size\n",
        "\n",
        "        yield data[idx0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTQd8_kWhG0e"
      },
      "source": [
        "# Load graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MnuTJZ05H0s",
        "scrolled": true
      },
      "source": [
        "full_dynamic_graph_sparse,unconnected_vertex_pairs,year_start,years_delta = pickle.load( open( data_source, \"rb\" ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "scrolled": true,
        "tags": [],
        "id": "5-Xi2K7ihG0h",
        "outputId": "0541dbba-d941-477d-c9f8-4bd593de5968"
      },
      "source": [
        "edges_used = 3000000#1*10**6 # Best would be to use all vertices, to create more training data. But that takes long and requires huge amount of memory. So here we use a random subset.\n",
        "vertex_degree_cutoff = 15\n",
        "_, train_edges_for_checking, train_edges_solution = create_training_data(\n",
        "    full_dynamic_graph_sparse, \n",
        "    year_start-years_delta, \n",
        "    years_delta, \n",
        "    edges_used=edges_used, \n",
        "    vertex_degree_cutoff=vertex_degree_cutoff)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Create Graph for  2014\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/anaconda3/envs/tensorflow2_p37/lib/python3.7/site-packages/scipy/sparse/_index.py:125: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_arrayXarray(i, j, x)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    num of edges:  1843252\n",
            "    Create Graph for  2017\n",
            "    num of edges:  5568238\n",
            "2883327\n",
            "Number of unconnected vertex pairs for prediction:  5758889\n",
            "Number of vertex pairs that will be connected:  2883327\n",
            "Ratio of vertex pairs that will be connected:  0.5006741751751076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkJw-vqZhG0j",
        "outputId": "a113af04-2bd8-4e57-d0d1-f2684dc1f2a6"
      },
      "source": [
        "np.random.seed(0)\n",
        "idx_col = np.random.choice([0,1], size=train_edges_for_checking.shape[0])\n",
        "np.random.seed(0)\n",
        "idx_row = np.random.choice(len(train_edges_for_checking), size=len(train_edges_for_checking), replace=False)\n",
        "train_edges_for_checking = np.array([train_edges_for_checking[idx_row, idx_col], \n",
        "                                     train_edges_for_checking[idx_row, 1-idx_col]]).T\n",
        "\n",
        "train_edges_solution = train_edges_solution[idx_row]\n",
        "print(train_edges_for_checking.shape, train_edges_solution.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5758889, 2) (5758889,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBHziVbPhG0k",
        "outputId": "0c85d9e4-512f-48a2-af17-ebbba8927da3"
      },
      "source": [
        "np.random.seed(0)\n",
        "idx_col = np.random.choice([0,1], size=unconnected_vertex_pairs.shape[0])\n",
        "idx_row = np.arange(len(unconnected_vertex_pairs))\n",
        "unconnected_vertex_pairs = np.array([unconnected_vertex_pairs[idx_row, idx_col], \n",
        "                                     unconnected_vertex_pairs[idx_row, 1-idx_col]]).T\n",
        "print(unconnected_vertex_pairs.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1000000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6PKQIq9hG0l"
      },
      "source": [
        "save_feature(train_edges_for_checking, 'edges')\n",
        "save_feature(train_edges_solution, 'train_solutions')\n",
        "save_feature(unconnected_vertex_pairs, 'unconnected_vertex_pairs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP8ERhXKhG0l",
        "outputId": "e649026e-5ad4-40d6-858d-ca4f2caeb56e"
      },
      "source": [
        "train_edges_for_checking[:10], unconnected_vertex_pairs[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[ 1748,  5950],\n",
              "        [51712, 31782],\n",
              "        [41359, 20995],\n",
              "        [53374, 54042],\n",
              "        [40188, 21390],\n",
              "        [47402, 33539],\n",
              "        [36194, 34151],\n",
              "        [36575, 34650],\n",
              "        [56348, 33309],\n",
              "        [45356, 35843]], dtype=int32),\n",
              " array([[40338, 18732],\n",
              "        [ 8023,  4221],\n",
              "        [ 4769,  2822],\n",
              "        [39823, 35173],\n",
              "        [15705, 33319],\n",
              "        [16627, 40028],\n",
              "        [ 9671, 39235],\n",
              "        [28625, 36765],\n",
              "        [35665, 17213],\n",
              "        [10643, 25217]], dtype=int32))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UQf8UiIzVB_"
      },
      "source": [
        "# Get network features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hew410x1hG0z"
      },
      "source": [
        "past = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "3rpim87XhG0z"
      },
      "source": [
        "train_years = range(year_start-years_delta, year_start-years_delta-past, -1)\n",
        "eval_years = range(year_start, year_start-past, -1)\n",
        "\n",
        "train_features, eval_features = [], []\n",
        "for yy in sorted(set(list(train_years)+list(eval_years)), reverse=True):      \n",
        "    adj = get_sparse_from_year(yy, full_dynamic_graph_sparse)\n",
        "    \n",
        "    print('year:', yy, 'edges:', adj.count_nonzero()//2)\n",
        "    \n",
        "    mats = compute_network_matrices(adj)\n",
        "    \n",
        "    if yy in train_years:\n",
        "        feat = compute_network_properties(mats, train_edges_for_checking)\n",
        "        train_features.extend(feat)\n",
        "    \n",
        "    if yy in eval_years:\n",
        "        feat = compute_network_properties(mats, unconnected_vertex_pairs)\n",
        "        eval_features.extend(feat)\n",
        "        \n",
        "train_features = np.array(train_features).T\n",
        "eval_features = np.array(eval_features).T\n",
        "\n",
        "print(train_features.shape, eval_features.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "W6fc4v3_hG00"
      },
      "source": [
        "save_feature(train_features, 'train_features')\n",
        "save_feature(eval_features, 'eval_features')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svnoZwazhG00"
      },
      "source": [
        "# Get embedding features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDH5q5YqhG01"
      },
      "source": [
        "past = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbtbmfLshG01"
      },
      "source": [
        "bin_ops = [operator_l2, operator_hadamard]\n",
        "\n",
        "train_years = range(year_start-years_delta, year_start-years_delta-past, -1)\n",
        "eval_years = range(year_start, year_start-past, -1)\n",
        "\n",
        "train_features, eval_features = [], []\n",
        "for yy in sorted(set(list(train_years)+list(eval_years)), reverse=True):    \n",
        "    print('year:', yy)\n",
        "    \n",
        "    if yy in train_years:\n",
        "        feat = compute_embedding_features(yy, train_edges_for_checking, bin_ops)\n",
        "        train_features.extend(feat)\n",
        "    \n",
        "    if yy in eval_years:\n",
        "        feat = compute_embedding_features(yy, unconnected_vertex_pairs, bin_ops)\n",
        "        eval_features.extend(feat)\n",
        "        \n",
        "train_features = np.array(train_features).T\n",
        "eval_features = np.array(eval_features).T\n",
        "\n",
        "print(train_features.shape, eval_features.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGsof5v6hG01"
      },
      "source": [
        "save_feature(train_features, 'embed_features')\n",
        "save_feature(eval_features, 'eval_embed_features')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQmkCwDUhG02"
      },
      "source": [
        "# Load all features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2Fp1elWhG02"
      },
      "source": [
        "train_edges_for_checking = load_feature('edges')\n",
        "train_edges_solution = load_feature('train_solutions')\n",
        "unconnected_vertex_pairs = load_feature('unconnected_vertex_pairs')\n",
        "\n",
        "train_features = load_feature('norm_features')\n",
        "eval_features = load_feature('norm_eval_features')\n",
        "\n",
        "embed_features = load_feature('norm_embed_features')\n",
        "eval_embed_features = load_feature('norm_eval_embed_features')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LADIZ9khG03"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoSvFMuohG03"
      },
      "source": [
        "(   idx_train,\n",
        "    idx_test,\n",
        "    labels_train,\n",
        "    labels_test,\n",
        ") = train_test_split(range(len(train_edges_solution)), train_edges_solution, train_size=0.95, test_size=0.05)\n",
        "\n",
        "data_train = np.hstack((train_features[idx_train], embed_features[idx_train]))\n",
        "data_test = np.hstack((train_features[idx_test], embed_features[idx_test]))\n",
        "data_eval = np.hstack((eval_features, eval_embed_features))\n",
        "\n",
        "print(data_train.shape, data_test.shape, data_eval.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMqcco1J5H04",
        "scrolled": true,
        "tags": []
      },
      "source": [
        "input_shape = (data_train.shape[1],)\n",
        "dropout_rate = 0.2\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Input(shape=input_shape),\n",
        "    keras.layers.Dense(512, activation='relu'),\n",
        "    keras.layers.Dropout(dropout_rate),\n",
        "    keras.layers.Dense(512, activation='relu'),\n",
        "    keras.layers.Dropout(dropout_rate),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dropout(dropout_rate),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=5e-4),\n",
        "        loss=keras.losses.binary_crossentropy,\n",
        "        metrics=[keras.metrics.binary_accuracy, keras.metrics.AUC()],\n",
        "    )\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "2GWqENQThG03"
      },
      "source": [
        "batch_size = 128\n",
        "\n",
        "model.fit(generate_train((data_train, labels_train),  batch_size),\n",
        "          steps_per_epoch = 30000,\n",
        "          epochs = 50,\n",
        "          verbose = 2,\n",
        "          callbacks = [keras.callbacks.EarlyStopping(monitor='loss', patience=1, restore_best_weights=True),\n",
        "                    keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=0, min_lr=5e-6)\n",
        "                    ],\n",
        "          use_multiprocessing=False\n",
        "         )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "4lqJzofrhG03"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "p = model.predict(generate_test(data_test, batch_size), \n",
        "                  steps=len(data_test)//batch_size+1, \n",
        "                  use_multiprocessing=False)\n",
        "\n",
        "print(roc_auc_score(labels_test, p))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55PVragKhG04"
      },
      "source": [
        "# Predict on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujbkO0BlhG04"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "p1 = model.predict(generate_test(data_eval, batch_size), \n",
        "                  steps=len(unconnected_vertex_pairs)//batch_size+1, \n",
        "                  use_multiprocessing=False)\n",
        "\n",
        "sorted_predictions_eval=np.flip(np.argsort(p1, axis=0))   \n",
        "print(p1[sorted_predictions_eval[0]], p1[sorted_predictions_eval[-1]])\n",
        "print(p1[:15])\n",
        "print(sum(p1>0.9)/len(unconnected_vertex_pairs)*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocJVzb55hG05"
      },
      "source": [
        "submit_file=os.path.join(DRIVE_PATH,\"pred\"+str(datetime.now())+\".json\")\n",
        "all_idx_list_float=list(map(float, sorted_predictions_eval))\n",
        "with open(submit_file, \"w\", encoding=\"utf8\") as json_file:\n",
        "    json.dump(all_idx_list_float, json_file)\n",
        "print(submit_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqTNkoyChG05"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}