{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "node_embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Environment (conda_tensorflow_p37)",
      "language": "python",
      "name": "conda_tensorflow_p37"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarlinLee/science4cast/blob/main/node_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5uxewDXkjur"
      },
      "source": [
        "%pip install -q stellargraph[demos]==1.2.1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfPvBFFE5qC2",
        "outputId": "be0bd0e2-7db3-4ce6-c02d-2a2cd2942959"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_PATH = \"/content/drive/My Drive/science4cast\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji_OypAWEjQV"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from math import isclose\n",
        "from scipy import sparse\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "from datetime import date\n",
        "from collections import Counter\n",
        "import multiprocessing\n",
        "from IPython.display import display, HTML\n",
        "from sklearn.model_selection import train_test_split\n",
        "import stellargraph as sg\n",
        "from stellargraph import StellarGraph, datasets\n",
        "from stellargraph.data import EdgeSplitter"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MnuTJZ05H0s",
        "scrolled": true,
        "outputId": "cf4c4b19-1276-44ab-d84e-cb710a6f9788"
      },
      "source": [
        "NUM_OF_VERTICES=64719 # number of vertices of the semantic net\n",
        "\n",
        "data_source = os.path.join(DRIVE_PATH, 'competition_data', 'CompetitionSet2017_3.pkl')\n",
        "#data_source = os.path.join(DRIVE_PATH, 'TrainSet2014_3.pkl')\n",
        "full_dynamic_graph_sparse,unconnected_vertex_pairs,year_start,years_delta = pickle.load( open( data_source, \"rb\" ) )\n",
        "\n",
        "print(data_source+' has '+str(len(full_dynamic_graph_sparse))+' edges between a total of '+str(NUM_OF_VERTICES)+ ' vertices.\\n\\n')\n",
        "print('The goal is to predict which of '+str(len(unconnected_vertex_pairs))+' unconnectedvertex-pairs\\nin unconnected_vertex_pairs will be connected until '+str(year_start+years_delta)+'.')\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/science4cast/competition_data/CompetitionSet2017_3.pkl has 7652945 edges between a total of 64719 vertices.\n",
            "\n",
            "\n",
            "The goal is to predict which of 1000000 unconnectedvertex-pairs\n",
            "in unconnected_vertex_pairs will be connected until 2020.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW6b1Yeu5H01",
        "scrolled": true
      },
      "source": [
        "def create_training_data(full_graph,year_start,years_delta,edges_used=500000,vertex_degree_cutoff=10):\n",
        "    \"\"\"\n",
        "    :param full_graph: Full graph, numpy array dim(n,3) [vertex 1, vertex 2, time stamp]\n",
        "    :param year_start: year of graph\n",
        "    :param years_delta: distance for prediction in years (prediction on graph of year_start+years_delta)\n",
        "    :param edges_used: optional filter to create a random subset of edges for rapid prototyping (default: 500,000)\n",
        "    :param vertex_degree_cutoff: optional filter, for vertices in training set having a minimal degree of at least vertex_degree_cutoff  (default: 10)\n",
        "    :return:\n",
        "\n",
        "    all_edge_list: graph of year_start, numpy array dim(n,2)\n",
        "    unconnected_vertex_pairs: potential edges for year_start+years_delta\n",
        "    unconnected_vertex_pairs_solution: numpy array with integers (0=unconnected, 1=connected), solution, length = len(unconnected_vertex_pairs)\n",
        "    \"\"\"\n",
        "\n",
        "    years=[year_start]    \n",
        "    day_origin = date(1990,1,1)\n",
        "\n",
        "    all_G=[]\n",
        "    all_edge_lists=[]\n",
        "    all_sparse=[]\n",
        "    for yy in years:\n",
        "        print('    Create Graph for ', yy)\n",
        "        day_curr=date(yy,12,31)\n",
        "        all_edges_curr=full_graph[full_graph[:,2]<(day_curr-day_origin).days]\n",
        "        adj_mat_sparse_curr = sparse.csr_matrix((np.ones(len(all_edges_curr)), (all_edges_curr[:,0], all_edges_curr[:,1])), shape=(NUM_OF_VERTICES,NUM_OF_VERTICES))\n",
        "        G_curr=nx.from_scipy_sparse_matrix(adj_mat_sparse_curr, parallel_edges=False, create_using=None, edge_attribute='weight')\n",
        "\n",
        "        all_G.append(G_curr)\n",
        "        all_sparse.append(adj_mat_sparse_curr)\n",
        "        all_edge_lists.append(all_edges_curr)\n",
        "\n",
        "        print('    Done: Create Graph for ', yy)\n",
        "        print('    num of edges: ', G_curr.number_of_edges())\n",
        "\n",
        "    all_degs=np.array(all_sparse[0].sum(0))[0]\n",
        "\n",
        "    unconnected_vertex_pairs=np.array([])\n",
        "    unconnected_vertex_pairs_solution=np.array([])\n",
        "\n",
        "    all_edge_list=np.array(all_edge_lists[0])\n",
        "    \n",
        "    return all_edge_list, unconnected_vertex_pairs, unconnected_vertex_pairs_solution\n",
        "\n",
        "edges_used=1*10**6 # Best would be to use all vertices, to create more training data. But that takes long and requires huge amount of memory. So here we use a random subset.\n",
        "vertex_degree_cutoff=10"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yav6-MDgIMRr",
        "outputId": "060abadd-9e3d-42ee-c366-8cb05ab21bb1"
      },
      "source": [
        "graphs = []\n",
        "years = [1994] # CHANGE ME TO DIFFERENT YEARS\n",
        "\n",
        "for yy in years:\n",
        "  train_dynamic_graph_sparse,train_edges_for_checking,train_edges_solution = create_training_data(full_dynamic_graph_sparse, yy, years_delta, edges_used=edges_used, vertex_degree_cutoff=vertex_degree_cutoff)\n",
        "\n",
        "  print(train_dynamic_graph_sparse[:,:-1].shape, train_edges_for_checking.shape, train_edges_solution.shape)\n",
        "  graph_train = StellarGraph(nodes=sg.IndexedArray(index=range(NUM_OF_VERTICES)), \n",
        "                          edges=pd.DataFrame(train_dynamic_graph_sparse[:,:-1], columns=[\"source\", \"target\"]))\n",
        "  print(graph_train.info())\n",
        "  graphs.append(graph_train)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Create Graph for  1994\n",
            "    Done: Create Graph for  1994\n",
            "    num of edges:  671\n",
            "(671, 2) (0,) (0,)\n",
            "StellarGraph: Undirected multigraph\n",
            " Nodes: 64719, Edges: 671\n",
            "\n",
            " Node types:\n",
            "  default: [64719]\n",
            "    Features: none\n",
            "    Edge types: default-default->default\n",
            "\n",
            " Edge types:\n",
            "    default-default->default: [671]\n",
            "        Weights: all 1 (default)\n",
            "        Features: none\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5z103PBDeg6"
      },
      "source": [
        "## Node Embedding\n",
        "\n",
        "https://stellargraph.readthedocs.io/en/stable/demos/link-prediction/node2vec-link-prediction.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xNaXXY_DtPu"
      },
      "source": [
        "from stellargraph.data import BiasedRandomWalk\n",
        "\n",
        "\n",
        "def create_biased_random_walker(graph, walk_num, walk_length):\n",
        "    # parameter settings for \"p\" and \"q\":\n",
        "    p = 2.0\n",
        "    q = 1.0\n",
        "    return BiasedRandomWalk(graph, n=walk_num, length=walk_length, p=p, q=q)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuS8GxW5D30I"
      },
      "source": [
        "walk_length = 5\n",
        "epochs = 100\n",
        "batch_size = 512"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-mPxpliEBiK"
      },
      "source": [
        "from stellargraph.data import UnsupervisedSampler\n",
        "from stellargraph.mapper import Node2VecLinkGenerator, Node2VecNodeGenerator\n",
        "from stellargraph.layer import Node2Vec, link_classification\n",
        "from tensorflow import keras\n",
        "\n",
        "def node2vec_embedding(graph, name, year_start):\n",
        "\n",
        "    # Set the embedding dimension and walk number:\n",
        "    dimension = 128\n",
        "    walk_number = 20\n",
        "\n",
        "    print(f\"Training Node2Vec for '{name}':\")\n",
        "    print(year_start)\n",
        "\n",
        "    graph_node_list = list(graph.nodes())\n",
        "\n",
        "    # Create the biased random walker to generate random walks\n",
        "    walker = create_biased_random_walker(graph, walk_number, walk_length)\n",
        "\n",
        "    # Create the unsupervised sampler to sample (target, context) pairs from random walks\n",
        "    unsupervised_samples = UnsupervisedSampler(\n",
        "        graph, nodes=graph_node_list, walker=walker\n",
        "    )\n",
        "\n",
        "    # Define a Node2Vec training generator, which generates batches of training pairs\n",
        "    generator = Node2VecLinkGenerator(graph, batch_size)\n",
        "\n",
        "    # Create the Node2Vec model\n",
        "    node2vec = Node2Vec(dimension, generator=generator)\n",
        "\n",
        "    # Build the model and expose input and output sockets of Node2Vec, for node pair inputs\n",
        "    x_inp, x_out = node2vec.in_out_tensors()\n",
        "\n",
        "    # Use the link_classification function to generate the output of the Node2Vec model\n",
        "    prediction = link_classification(\n",
        "        output_dim=1, output_act=\"sigmoid\", edge_embedding_method=\"dot\"\n",
        "    )(x_out)\n",
        "\n",
        "    # Stack the Node2Vec encoder and prediction layer into a Keras model, and specify the loss\n",
        "    model = keras.Model(inputs=x_inp, outputs=prediction)\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss=keras.losses.binary_crossentropy,\n",
        "        metrics=[keras.metrics.binary_accuracy],\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(\n",
        "        generator.flow(unsupervised_samples),\n",
        "        epochs=epochs,\n",
        "        verbose=2,\n",
        "        callbacks = [keras.callbacks.EarlyStopping(monitor='loss', patience=2, restore_best_weights=True),\n",
        "                     keras.callbacks.ModelCheckpoint(filepath=os.path.join(DRIVE_PATH,'node2vec'+str(date.today())+'.h5'),\n",
        "                                                     monitor='loss',save_best_only=True),\n",
        "                    keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
        "                              patience=1, min_lr=1e-4)],\n",
        "        use_multiprocessing=True,\n",
        "        workers=8,\n",
        "    )\n",
        "\n",
        "    # Build the model to predict node representations from node ids with the learned Node2Vec model parameters\n",
        "    x_inp_src = x_inp[0]\n",
        "    x_out_src = x_out[0]\n",
        "    embedding_model = keras.Model(inputs=x_inp_src, outputs=x_out_src)\n",
        "\n",
        "    # Get representations for all nodes in ``graph``\n",
        "    node_gen = Node2VecNodeGenerator(graph, batch_size).flow(graph_node_list)\n",
        "    node_embeddings = embedding_model.predict(node_gen, workers=8, verbose=2, use_multiprocessing=True)\n",
        "    \n",
        "    with open(os.path.join(DRIVE_PATH, 'node2vec-'+str(year_start)+'.pkl'), \"wb\") as output_file:\n",
        "        pickle.dump(node_embeddings, output_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    return node_embeddings"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RJqlTn5Fw1a",
        "outputId": "755831bd-aa54-46d3-91c4-2ed1985ceaca"
      },
      "source": [
        "for yy in years:\n",
        "  node_embeddings = node2vec_embedding(graph_train, \"Node2Vec\", yy)  "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Node2Vec for 'Node2Vec':\n",
            "1994\n",
            "link_classification: using 'dot' method to combine node embeddings into edge embeddings\n",
            "Epoch 1/100\n",
            "37/37 - 12s - loss: 0.6877 - binary_accuracy: 0.5537 - lr: 0.0010 - 12s/epoch - 329ms/step\n",
            "Epoch 2/100\n",
            "37/37 - 13s - loss: 0.6345 - binary_accuracy: 0.6322 - lr: 0.0010 - 13s/epoch - 343ms/step\n",
            "Epoch 3/100\n",
            "37/37 - 12s - loss: 0.5954 - binary_accuracy: 0.6862 - lr: 0.0010 - 12s/epoch - 317ms/step\n",
            "Epoch 4/100\n",
            "37/37 - 13s - loss: 0.5669 - binary_accuracy: 0.7137 - lr: 0.0010 - 13s/epoch - 350ms/step\n",
            "Epoch 5/100\n",
            "37/37 - 12s - loss: 0.5394 - binary_accuracy: 0.7363 - lr: 0.0010 - 12s/epoch - 321ms/step\n",
            "Epoch 6/100\n",
            "37/37 - 11s - loss: 0.5183 - binary_accuracy: 0.7533 - lr: 0.0010 - 11s/epoch - 287ms/step\n",
            "Epoch 7/100\n",
            "37/37 - 11s - loss: 0.5004 - binary_accuracy: 0.7668 - lr: 0.0010 - 11s/epoch - 299ms/step\n",
            "Epoch 8/100\n",
            "37/37 - 11s - loss: 0.4831 - binary_accuracy: 0.7865 - lr: 0.0010 - 11s/epoch - 309ms/step\n",
            "Epoch 9/100\n",
            "37/37 - 12s - loss: 0.4681 - binary_accuracy: 0.7988 - lr: 0.0010 - 12s/epoch - 314ms/step\n",
            "Epoch 10/100\n",
            "37/37 - 11s - loss: 0.4524 - binary_accuracy: 0.8110 - lr: 0.0010 - 11s/epoch - 304ms/step\n",
            "Epoch 11/100\n",
            "37/37 - 11s - loss: 0.4408 - binary_accuracy: 0.8263 - lr: 0.0010 - 11s/epoch - 292ms/step\n",
            "Epoch 12/100\n",
            "37/37 - 10s - loss: 0.4263 - binary_accuracy: 0.8376 - lr: 0.0010 - 10s/epoch - 282ms/step\n",
            "Epoch 13/100\n",
            "37/37 - 10s - loss: 0.4111 - binary_accuracy: 0.8503 - lr: 0.0010 - 10s/epoch - 270ms/step\n",
            "Epoch 14/100\n",
            "37/37 - 11s - loss: 0.3988 - binary_accuracy: 0.8606 - lr: 0.0010 - 11s/epoch - 309ms/step\n",
            "Epoch 15/100\n",
            "37/37 - 12s - loss: 0.3896 - binary_accuracy: 0.8652 - lr: 0.0010 - 12s/epoch - 314ms/step\n",
            "Epoch 16/100\n",
            "37/37 - 11s - loss: 0.3753 - binary_accuracy: 0.8749 - lr: 0.0010 - 11s/epoch - 309ms/step\n",
            "Epoch 17/100\n",
            "37/37 - 10s - loss: 0.3600 - binary_accuracy: 0.8843 - lr: 0.0010 - 10s/epoch - 283ms/step\n",
            "Epoch 18/100\n",
            "37/37 - 10s - loss: 0.3499 - binary_accuracy: 0.8869 - lr: 0.0010 - 10s/epoch - 280ms/step\n",
            "Epoch 19/100\n",
            "37/37 - 12s - loss: 0.3288 - binary_accuracy: 0.8988 - lr: 0.0010 - 12s/epoch - 323ms/step\n",
            "Epoch 20/100\n",
            "37/37 - 11s - loss: 0.3203 - binary_accuracy: 0.8979 - lr: 0.0010 - 11s/epoch - 292ms/step\n",
            "Epoch 21/100\n",
            "37/37 - 11s - loss: 0.3135 - binary_accuracy: 0.9022 - lr: 0.0010 - 11s/epoch - 298ms/step\n",
            "Epoch 22/100\n",
            "37/37 - 11s - loss: 0.2979 - binary_accuracy: 0.9050 - lr: 0.0010 - 11s/epoch - 294ms/step\n",
            "Epoch 23/100\n",
            "37/37 - 10s - loss: 0.2913 - binary_accuracy: 0.9073 - lr: 0.0010 - 10s/epoch - 283ms/step\n",
            "Epoch 24/100\n",
            "37/37 - 11s - loss: 0.2836 - binary_accuracy: 0.9076 - lr: 0.0010 - 11s/epoch - 301ms/step\n",
            "Epoch 25/100\n",
            "37/37 - 10s - loss: 0.2754 - binary_accuracy: 0.9110 - lr: 0.0010 - 10s/epoch - 272ms/step\n",
            "Epoch 26/100\n",
            "37/37 - 10s - loss: 0.2686 - binary_accuracy: 0.9112 - lr: 0.0010 - 10s/epoch - 270ms/step\n",
            "Epoch 27/100\n",
            "37/37 - 11s - loss: 0.2684 - binary_accuracy: 0.9089 - lr: 0.0010 - 11s/epoch - 296ms/step\n",
            "Epoch 28/100\n",
            "37/37 - 12s - loss: 0.2663 - binary_accuracy: 0.9090 - lr: 0.0010 - 12s/epoch - 333ms/step\n",
            "Epoch 29/100\n",
            "37/37 - 11s - loss: 0.2580 - binary_accuracy: 0.9113 - lr: 0.0010 - 11s/epoch - 311ms/step\n",
            "Epoch 30/100\n",
            "37/37 - 11s - loss: 0.2542 - binary_accuracy: 0.9101 - lr: 0.0010 - 11s/epoch - 293ms/step\n",
            "Epoch 31/100\n",
            "37/37 - 10s - loss: 0.2483 - binary_accuracy: 0.9132 - lr: 0.0010 - 10s/epoch - 271ms/step\n",
            "Epoch 32/100\n",
            "37/37 - 13s - loss: 0.2414 - binary_accuracy: 0.9163 - lr: 0.0010 - 13s/epoch - 362ms/step\n",
            "Epoch 33/100\n",
            "37/37 - 7s - loss: 0.2458 - binary_accuracy: 0.9117 - lr: 0.0010 - 7s/epoch - 186ms/step\n",
            "Epoch 34/100\n",
            "37/37 - 6s - loss: 0.2449 - binary_accuracy: 0.9123 - lr: 2.0000e-04 - 6s/epoch - 174ms/step\n",
            "127/127 - 25s - 25s/epoch - 198ms/step\n"
          ]
        }
      ]
    }
  ]
}